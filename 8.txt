import os
import pickle
import pandas as pd
import numpy as np
from tkinter import *
from tkinter import filedialog
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, normalize
from keras.models import Sequential, model_from_json
from keras.layers import LSTM, Dense, Dropout
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from string import punctuation

nltk.download('stopwords')
nltk.download('wordnet')

# Global Variables
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
textdata, labels = [], []
classifier = None

def cleanPost(doc):
    tokens = [lemmatizer.lemmatize(word) for word in doc.lower().split() if word.isalpha() and word not in stop_words]
    return ' '.join(tokens)

def uploadDataset():
    global textdata, labels
    filename = filedialog.askopenfilename()
    dataset = pd.read_csv(filename).fillna(' ')
    textdata = [cleanPost(str(row['text'])) for _, row in dataset.iterrows()]
    labels = dataset['target'].values

def preprocess():
    global X, Y, tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test
    tfidf = TfidfVectorizer(max_features=200).fit_transform(textdata).toarray()
    X = normalize(tfidf)
    Y = LabelEncoder().fit_transform(labels).reshape(-1, 1)
    X = X.reshape((X.shape[0], X.shape[1], 1))
    tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test = train_test_split(X, Y, test_size=0.2)

def runLSTM():
    global classifier
    if os.path.exists('model/model.json'):
        with open('model/model.json', "r") as json_file:
            classifier = model_from_json(json_file.read())
        classifier.load_weights("model/model_weights.h5")
    else:
        lstm_model = Sequential([
            LSTM(128, activation='relu', return_sequences=True, input_shape=(X.shape[1:])),
            Dropout(0.2),
            LSTM(128, activation='relu'),
            Dropout(0.2),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(2, activation='softmax')
        ])
        lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        lstm_model.fit(X, Y, epochs=10, validation_data=(tfidf_X_test, tfidf_y_test))
        classifier = lstm_model
        lstm_model.save_weights('model/model_weights.h5')
        with open("model/model.json", "w") as json_file:
            json_file.write(lstm_model.to_json())

def predict():
    testfile = filedialog.askopenfilename()
    testData = pd.read_csv(testfile)['text'].values
    for msg in testData:
        clean_msg = cleanPost(msg)
        testReview = TfidfVectorizer(max_features=200).fit_transform([clean_msg]).toarray()
        predict = classifier.predict(testReview)
        print(f"{msg} === {'GENUINE' if predict == 0 else 'FAKE'}")

# GUI Setup
main = Tk()
main.title("Fake News Detection")
main.geometry("1200x600")

Button(main, text="Upload Dataset", command=uploadDataset).pack()
Button(main, text="Preprocess Data", command=preprocess).pack()
Button(main, text="Run LSTM", command=runLSTM).pack()
Button(main, text="Predict News", command=predict).pack()

main.mainloop()